# ai_data_awareness.py - AI Data Awareness System
# Location: /garage_manager/services/ai_data_awareness.py
# Description: AI data and structural awareness system


import os
import json
from datetime import datetime
from pathlib import Path
from sqlalchemy import inspect
from sqlalchemy.orm import class_mapper


DATA_SCHEMA_FILE = 'instance/ai_data_schema.json'
LEARNING_LOG_FILE = 'instance/ai_learning_log.json'


def discover_all_models():
    try:
        from models import (
            User, Customer, Supplier, Product, ServiceRequest, Invoice, Payment,
            Warehouse, StockLevel, Note, Shipment, AuditLog, Role, Permission,
            ExchangeTransaction, Expense, ExpenseType, Account, Partner,
            PartnerSettlement, SupplierSettlement, PreOrder, OnlineCart,
            ServicePart, ServiceTask, Currency, ProductRating, SystemSettings
        )
        
        models = [
            User, Customer, Supplier, Product, ServiceRequest, Invoice, Payment,
            Warehouse, StockLevel, Note, Shipment, AuditLog, Role, Permission,
            ExchangeTransaction, Expense, ExpenseType, Account, Partner,
            PartnerSettlement, SupplierSettlement, PreOrder, OnlineCart,
            ServicePart, ServiceTask, Currency, ProductRating, SystemSettings
        ]
        
        return models
    
    except Exception as e:
        print(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ù†Ù…Ø§Ø°Ø¬: {str(e)}")
        return []


def analyze_model_structure(model):
    """ØªØ­Ù„ÙŠÙ„ Ø¨Ù†ÙŠØ© Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ø­Ø¯"""
    try:
        mapper = class_mapper(model)
        columns = []
        relationships = []
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©
        for column in mapper.columns:
            col_info = {
                'name': column.name,
                'type': str(column.type),
                'nullable': column.nullable,
                'primary_key': column.primary_key,
                'foreign_key': len(column.foreign_keys) > 0,
            }
            columns.append(col_info)
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª
        for rel in mapper.relationships:
            rel_info = {
                'name': rel.key,
                'target': rel.mapper.class_.__name__,
                'uselist': rel.uselist,  # Many or One
                'type': 'one-to-many' if rel.uselist else 'many-to-one'
            }
            relationships.append(rel_info)
        
        return {
            'table_name': mapper.local_table.name,
            'class_name': model.__name__,
            'columns_count': len(columns),
            'columns': columns,
            'relationships_count': len(relationships),
            'relationships': relationships,
        }
    
    except Exception as e:
        return {
            'table_name': 'unknown',
            'class_name': model.__name__ if hasattr(model, '__name__') else 'unknown',
            'error': str(e)
        }


def build_functional_mapping():
    """Ø¨Ù†Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ø§Ù„ÙˆØ¹ÙŠ Ø§Ù„ÙˆØ¸ÙŠÙÙŠ"""
    return {
        'Ø§Ù„ØµÙŠØ§Ù†Ø©': {
            'models': ['ServiceRequest', 'ServicePart', 'ServiceTask'],
            'primary_table': 'service_request',
            'purpose': 'Ø¥Ø¯Ø§Ø±Ø© Ø·Ù„Ø¨Ø§Øª Ø§Ù„ØµÙŠØ§Ù†Ø© ÙˆÙ‚Ø·Ø¹ Ø§Ù„ØºÙŠØ§Ø± ÙˆØ§Ù„Ù…Ù‡Ø§Ù…',
            'keywords': ['ØµÙŠØ§Ù†Ø©', 'Ø¥ØµÙ„Ø§Ø­', 'Ø¹Ø·Ù„', 'ØªØ´Ø®ÙŠØµ', 'workshop', 'service']
        },
        'Ø§Ù„Ù†ÙÙ‚Ø§Øª': {
            'models': ['Expense', 'ExpenseType'],
            'primary_table': 'expense',
            'purpose': 'ØªØªØ¨Ø¹ Ø§Ù„Ù…ØµØ§Ø±ÙŠÙ ÙˆØ§Ù„Ù†ÙÙ‚Ø§Øª',
            'keywords': ['Ù†ÙÙ‚Ø©', 'Ù…ØµØ±ÙˆÙ', 'Ù…ØµØ§Ø±ÙŠÙ', 'expense']
        },
        'Ø§Ù„Ù…Ø­Ø§Ø³Ø¨Ø©': {
            'models': ['Account', 'ExchangeTransaction'],
            'primary_table': 'account',
            'purpose': 'Ø¥Ø¯Ø§Ø±Ø© Ø¯ÙØªØ± Ø§Ù„Ø£Ø³ØªØ§Ø° ÙˆØ§Ù„Ø­Ø³Ø§Ø¨Ø§Øª',
            'keywords': ['Ø¯ÙØªØ±', 'Ø­Ø³Ø§Ø¨', 'Ù…Ø­Ø§Ø³Ø¨Ø©', 'ledger', 'accounting']
        },
        'Ø§Ù„Ù…ØªØ¬Ø±': {
            'models': ['Product', 'OnlineCart', 'PreOrder', 'ProductRating'],
            'primary_table': 'product',
            'purpose': 'Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª ÙˆØ§Ù„Ù…ØªØ¬Ø± Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ',
            'keywords': ['Ù…ØªØ¬Ø±', 'Ù…Ù†ØªØ¬', 'Ø·Ù„Ø¨', 'Ø³Ù„Ø©', 'shop', 'store', 'product']
        },
        'Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª': {
            'models': ['Invoice', 'Payment'],
            'primary_table': 'invoice',
            'purpose': 'Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„ÙÙˆØ§ØªÙŠØ± ÙˆØ§Ù„Ù…Ø¯ÙÙˆØ¹Ø§Øª',
            'keywords': ['ÙØ§ØªÙˆØ±Ø©', 'Ø¯ÙØ¹', 'Ù…Ø¨ÙŠØ¹Ø§Øª', 'invoice', 'payment', 'sales']
        },
        'Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡': {
            'models': ['Customer'],
            'primary_table': 'customer',
            'purpose': 'Ø¥Ø¯Ø§Ø±Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡',
            'keywords': ['Ø¹Ù…ÙŠÙ„', 'Ø²Ø¨ÙˆÙ†', 'customer', 'client']
        },
        'Ø§Ù„Ù…ÙˆØ±Ø¯ÙŠÙ†': {
            'models': ['Supplier', 'SupplierSettlement'],
            'primary_table': 'supplier',
            'purpose': 'Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…ÙˆØ±Ø¯ÙŠÙ† ÙˆØ§Ù„Ù…Ø´ØªØ±ÙŠØ§Øª',
            'keywords': ['Ù…ÙˆØ±Ø¯', 'Ø´Ø±Ø§Ø¡', 'supplier', 'vendor']
        },
        'Ø§Ù„Ù…Ø®Ø§Ø²Ù†': {
            'models': ['Warehouse', 'StockLevel', 'Shipment'],
            'primary_table': 'warehouse',
            'purpose': 'Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…Ø®Ø²ÙˆÙ† ÙˆØ§Ù„Ø´Ø­Ù†Ø§Øª',
            'keywords': ['Ù…Ø®Ø²Ù†', 'Ù…Ø®Ø²ÙˆÙ†', 'Ø´Ø­Ù†Ø©', 'warehouse', 'stock', 'inventory']
        },
        'Ø§Ù„Ø´Ø±ÙƒØ§Ø¡': {
            'models': ['Partner', 'PartnerSettlement'],
            'primary_table': 'partner',
            'purpose': 'Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø´Ø±Ø§ÙƒØ§Øª ÙˆØ§Ù„ØªØ³ÙˆÙŠØ§Øª',
            'keywords': ['Ø´Ø±ÙŠÙƒ', 'Ø´Ø±Ø§ÙƒØ©', 'ØªØ³ÙˆÙŠØ©', 'partner', 'settlement']
        },
        'Ø§Ù„Ø¶Ø±Ø§Ø¦Ø¨ ÙˆØ§Ù„Ø¹Ù…Ù„Ø§Øª': {
            'models': ['ExchangeTransaction', 'Currency'],
            'primary_table': 'exchange_transaction',
            'purpose': 'Ø¥Ø¯Ø§Ø±Ø© Ø£Ø³Ø¹Ø§Ø± Ø§Ù„ØµØ±Ù ÙˆØ§Ù„Ø¶Ø±Ø§Ø¦Ø¨',
            'keywords': ['Ø¶Ø±ÙŠØ¨Ø©', 'ØµØ±Ù', 'Ø¹Ù…Ù„Ø©', 'Ø¯ÙˆÙ„Ø§Ø±', 'tax', 'exchange', 'currency']
        },
        'Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† ÙˆØ§Ù„Ø£Ù…Ø§Ù†': {
            'models': ['User', 'Role', 'Permission', 'AuditLog'],
            'primary_table': 'user',
            'purpose': 'Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† ÙˆØ§Ù„ØµÙ„Ø§Ø­ÙŠØ§Øª',
            'keywords': ['Ù…Ø³ØªØ®Ø¯Ù…', 'ØµÙ„Ø§Ø­ÙŠØ©', 'Ø¯ÙˆØ±', 'user', 'role', 'permission', 'audit']
        },
        'Ø§Ù„Ù…Ù„Ø§Ø­Ø¸Ø§Øª': {
            'models': ['Note'],
            'primary_table': 'note',
            'purpose': 'Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…Ù„Ø§Ø­Ø¸Ø§Øª ÙˆØ§Ù„Ù…Ø°ÙƒØ±Ø§Øª',
            'keywords': ['Ù…Ù„Ø§Ø­Ø¸Ø©', 'Ù…Ø°ÙƒØ±Ø©', 'note']
        }
    }


def build_language_mapping():
    """Ø¨Ù†Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ø§Ù„ØªØ±Ø¬Ù…Ø© Ø§Ù„Ù„ØºÙˆÙŠØ©"""
    return {
        'Ù…Ø¨ÙŠØ¹Ø§Øª': ['sales', 'invoice', 'payment'],
        'Ø¯ÙØªØ±': ['ledger', 'account'],
        'Ù†ÙÙ‚Ø§Øª': ['expense', 'expenses'],
        'Ø¶Ø±Ø§Ø¦Ø¨': ['tax', 'vat'],
        'Ø³Ø¹Ø± Ø§Ù„Ø¯ÙˆÙ„Ø§Ø±': ['exchange', 'usd', 'ils'],
        'Ø¹Ù…Ù„Ø§Ø¡': ['customer', 'client'],
        'Ù…ÙˆØ±Ø¯ÙŠÙ†': ['supplier', 'vendor'],
        'Ù…ØªØ¬Ø±': ['shop', 'store', 'product'],
        'ØµÙŠØ§Ù†Ø©': ['service', 'workshop', 'repair'],
        'Ù…Ø®Ø§Ø²Ù†': ['warehouse', 'inventory', 'stock'],
        'Ø´Ø±ÙƒØ§Ø¡': ['partner', 'partnership'],
    }


def build_data_schema():
    """Ø¨Ù†Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙƒØ§Ù…Ù„Ø©"""
    print("\nðŸ§  Ø¨Ø¯Ø¡ Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙˆØ¹ÙŠ Ø§Ù„Ø¨Ù†ÙŠÙˆÙŠ...")
    
    models = discover_all_models()
    print(f"âœ… ØªÙ… Ø§ÙƒØªØ´Ø§Ù {len(models)} Ù†Ù…ÙˆØ°Ø¬")
    
    schema = {
        'generated_at': datetime.now().isoformat(),
        'models_count': len(models),
        'models': {},
        'functional_mapping': build_functional_mapping(),
        'language_mapping': build_language_mapping(),
        'statistics': {
            'total_tables': 0,
            'total_columns': 0,
            'total_relationships': 0,
        }
    }
    
    total_columns = 0
    total_relationships = 0
    
    for model in models:
        analysis = analyze_model_structure(model)
        
        if 'error' not in analysis:
            schema['models'][model.__name__] = analysis
            total_columns += analysis['columns_count']
            total_relationships += analysis['relationships_count']
    
    schema['statistics']['total_tables'] = len(schema['models'])
    schema['statistics']['total_columns'] = total_columns
    schema['statistics']['total_relationships'] = total_relationships
    
    save_data_schema(schema)
    log_learning_event('schema_built', len(models))
    
    print(f"\nâœ… Ø§ÙƒØªÙ…Ù„ Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙˆØ¹ÙŠ Ø§Ù„Ø¨Ù†ÙŠÙˆÙŠ!")
    print(f"ðŸ“Š Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª:")
    print(f"   â€¢ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„: {len(schema['models'])}")
    print(f"   â€¢ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©: {total_columns}")
    print(f"   â€¢ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª: {total_relationships}")
    
    return schema


def save_data_schema(schema):
    """Ø­ÙØ¸ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
    try:
        os.makedirs('instance', exist_ok=True)
        
        with open(DATA_SCHEMA_FILE, 'w', encoding='utf-8') as f:
            json.dump(schema, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ø®Ø±ÙŠØ·Ø© ÙÙŠ {DATA_SCHEMA_FILE}")
    
    except Exception as e:
        print(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ø§Ù„Ø®Ø±ÙŠØ·Ø©: {str(e)}")


def load_data_schema():
    """ØªØ­Ù…ÙŠÙ„ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
    try:
        if os.path.exists(DATA_SCHEMA_FILE):
            with open(DATA_SCHEMA_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        else:
            print("âš ï¸ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© - Ø³ÙŠØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡Ø§ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹")
            return None
    
    except Exception as e:
        print(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø®Ø±ÙŠØ·Ø©: {str(e)}")
        return None


def log_learning_event(event_type, details):
    """ØªØ³Ø¬ÙŠÙ„ Ø­Ø¯Ø« Ø§Ù„ØªØ¹Ù„Ù…"""
    try:
        log_entry = {
            'timestamp': datetime.now().isoformat(),
            'event': event_type,
            'details': details,
        }
        
        logs = []
        if os.path.exists(LEARNING_LOG_FILE):
            with open(LEARNING_LOG_FILE, 'r', encoding='utf-8') as f:
                logs = json.load(f)
        
        logs.append(log_entry)
        logs = logs[-100:]  # Ø¢Ø®Ø± 100 Ø­Ø¯Ø«
        
        with open(LEARNING_LOG_FILE, 'w', encoding='utf-8') as f:
            json.dump(logs, f, ensure_ascii=False, indent=2)
    
    except Exception as e:
        print(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø­Ø¯Ø«: {str(e)}")


def find_model_by_keyword(keyword):
    """Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù†Ù…ÙˆØ°Ø¬ Ø­Ø³Ø¨ ÙƒÙ„Ù…Ø© Ù…ÙØªØ§Ø­ÙŠØ©"""
    schema = load_data_schema()
    
    if not schema:
        return None
    
    keyword_lower = keyword.lower()
    matches = []
    
    # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ø®Ø±ÙŠØ·Ø© Ø§Ù„ÙˆØ¸ÙŠÙÙŠØ©
    for module_name, module_data in schema['functional_mapping'].items():
        if any(k in keyword_lower for k in module_data['keywords']):
            matches.append({
                'module': module_name,
                'models': module_data['models'],
                'purpose': module_data['purpose']
            })
    
    # Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø¨Ø§Ø´Ø± ÙÙŠ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
    for model_name in schema['models'].keys():
        if keyword_lower in model_name.lower():
            matches.append({
                'model': model_name,
                'table': schema['models'][model_name]['table_name']
            })
    
    return matches


def auto_build_if_needed():
    """Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø®Ø±ÙŠØ·Ø© Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±"""
    if not os.path.exists(DATA_SCHEMA_FILE):
        print("ðŸ”„ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª - Ø³ÙŠØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡Ø§...")
        return build_data_schema()
    
    # ÙØ­Øµ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø®Ø±ÙŠØ·Ø© Ù‚Ø¯ÙŠÙ…Ø© (Ø£ÙƒØ«Ø± Ù…Ù† 7 Ø£ÙŠØ§Ù…)
    try:
        file_time = os.path.getmtime(DATA_SCHEMA_FILE)
        age_days = (datetime.now().timestamp() - file_time) / (3600 * 24)
        
        if age_days > 7:
            print(f"ðŸ”„ Ø§Ù„Ø®Ø±ÙŠØ·Ø© Ù‚Ø¯ÙŠÙ…Ø© ({age_days:.1f} ÙŠÙˆÙ…) - Ø³ÙŠØªÙ… ØªØ­Ø¯ÙŠØ«Ù‡Ø§...")
            return build_data_schema()
    
    except:
        pass
    
    return load_data_schema()


if __name__ == '__main__':
    print("ðŸ§ª Ø§Ø®ØªØ¨Ø§Ø± Ù†Ø¸Ø§Ù… Ø§Ù„ÙˆØ¹ÙŠ Ø§Ù„Ø¨Ù†ÙŠÙˆÙŠ...")
    schema = build_data_schema()
    print(f"\nâœ… ØªÙ… Ø¨Ù†Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ù„Ù€ {len(schema['models'])} Ù†Ù…ÙˆØ°Ø¬")

